\documentclass[a4paper,oneside,12pt]{report}

%%%%%%%%%%%%%%%%%%% LOAD PACKAGES %%%%%%%%%%%%%%%%%%%%%
%Get support for äöüßÄÖÜ
\usepackage[utf8]{inputenc}
%Get support for word separation
\usepackage[T1]{fontenc}
%Direkte Verwendung von Umlauten
\usepackage[ansinew]{luainputenc}
%Get Support for US english and german
\usepackage[english,ngerman]{babel}
%verbesserter Randausgleich
\usepackage{microtype}
%zulässiger Wortzwischenraum erhöhen für noch besseren Randausgleich und Blocksatzbildung
\setlength\emergencystretch{1em}
%Einrücken nach Absatz für gesamtes Dokument abstellen
\setlength{\parindent}{0pt} 
%Bildbeschreibung zentrieren
\usepackage[center]{caption}
%Support for Graphics .png .jpg .pdf 
\usepackage{graphicx}
%Zusätzliche Option[H] bei Bildern - Hier und und sonst nirgends!
\usepackage{float}
%Adjust the page margins asymmetrical, Need to be before subfig
\usepackage[left=3.5cm, right=2.5cm, top=3cm, bottom=3cm, a4paper, portrait]{geometry}
%Kopf- und Fußzeilen
\usepackage{fancyhdr}
%für Fußnote (linksbündig)
\usepackage[flushmargin,bottom]{footmisc}
%Erweiterung des Mathematikmoduses
\usepackage{amsmath}
%Use Palatino Linotype 
\usepackage{mathpazo}
%Don´t show "Kapitel 1" at \chapter
\usepackage{titlesec} 
\titleformat{\chapter}{\bfseries\Huge}{\thechapter\quad}{0em}{}
%Abstand Kapitelüberschiften zum Kopfrand
\titlespacing{\chapter}{0pt}{*-2}{*1.5}
\usepackage{setspace}
\AtBeginDocument{\renewcommand{\chaptername}{}}
%Tabellenspalten/zeilen einfärben
\usepackage{colortbl}
%variable Tabellenbreite
\usepackage{tabularx}
%Multirow in Tabelle
\usepackage{multirow}
%Tabellenüberschrift links oben!
\usepackage[singlelinecheck=false]{caption}
%Programmcode einfügen
\usepackage{listings}
%Programmcode Farben ändern
\usepackage{xcolor}
%Hyperlink
\usepackage[colorlinks=true,linkcolor=black, citecolor=black, urlcolor=black]{hyperref}
%Ordnerstruktur erstellen
\usepackage{dirtree}
\usepackage{subfigure}
%Abkuerzungen
\usepackage{acronym}
%Highlighting
\usepackage{color,soul}
%Unterstützung für textumflossenes Bild 
\usepackage{wrapfig}


%//%%%%%%%%%%%%%%%%%% END LOAD PACKAGES %%%%%%%%%%%%%%%%%%%%
%//%%%%%%%%%%%%%%%%%% SETTINGS %%%%%%%%%%%%%%%%%%%%%%%%%
%Adjust headings and footers --> \usepackage{fancyhdr}
%Give the headings some space
\setlength{\headheight}{20pt}
%This is valid for all pages exept chapters
\pagestyle{fancy}

\fancyhf{} % clear all headers and footers
%Ausrichtung Kopf- /Fußzeile
\lhead[]{\fancyplain{}{\leftmark}}
\cfoot[]{\thepage}
\renewcommand{\headrulewidth}{0.4pt}
%For \chapters \maketitle
\fancypagestyle{plain}{%
	\fancyhf{} % clear all header and footer fields
	\cfoot[]{\thepage}%
	\renewcommand{\headrulewidth}{0pt}
	\renewcommand{\footrulewidth}{0pt}
}
%Deactivate command --> small letter
\let\MakeUppercase\relax
%Tiefe von Inhaltsverzeichnis 
\setcounter{secnumdepth}{3}
\setcounter{tocdepth}{3}
%Programmcodeeinstellungen
\lstset{
	backgroundcolor=\color{black!5},
	tabsize=4,
	language=C++,
	captionpos=b,
	tabsize=3,
	frame=lines,
	numbers=left,
	numberstyle=\tiny,
	numbersep=5pt,
	breaklines=true,
	showstringspaces=false,
	basicstyle=\tiny,
	%identifierstyle=\color{magenta},
	keywordstyle=\color[rgb]{0.8,0.4,0},
	commentstyle=\color[rgb]{0.9,0.3,0.9,},
	stringstyle=\color{green},
	emphstyle=\color{blue},
}
%Programmcodeeinstellungen, style Arduino
	\lstdefinestyle{Arduino}{%
		%style=numbers,
		keywords={soup},%                 define keywords
		%morestring=[s]{<}{>},%			  define <> as strings
		morecomment=[l]{\#},%             treat // as comments
		morecomment=[s]{/*}{*/},%         define /* ... */ comments
		emph={void, for, else, if, in, HIGH, OUTPUT, LOW, int, uint8_t, self, def, True, False, print, from, import, as, class, try, except, not, or, return, global},%        keywords to emphasize	
}


%Bennenung von Caption bei Programmcode 
	\renewcommand{\lstlistingname}{Programmcode}
	\renewcommand{\lstlistlistingname}{\lstlistingname}
	%Bennenung von Bilderquelle als "Quelle:"
	\newcommand*{\quelle}{%
		\footnotesize Quelle:
	}

%//%%%%%%%%%%%%%%%%%% END SETTINGS %%%%%%%%%%%%%%%%%%%%%%%%%

%\\%%%%%%%%%%%%%%%%%% DOCUMENT %%%%%%%%%%%%%%%%%%%%%%%%%


\begin{document}
	\include{Deckblatt/deckblatt}
	\pagenumbering{Roman}


%												 Inhaltsverzeichnis
	\newpage
	\tableofcontents
	
	
	
	
	
%											    Kapitel 1 - Einleitung
	%\newpage
	\pagenumbering{arabic}
	\setstretch{1.5}
	\chapter{Problemstellung}\label{probstellung}
	
		Für nahezu Jedermann bietet das WWW im Vergleich zu damaligen Zeiten ungeahnte Möglichkeiten der Recherche und Themenfindung. Noch nie waren so viele Informationen öffentlich einsehbar wie heute. Meist sind jedoch die gewünschten Inhalte an vielen unterschiedlichen Orten im Web verteilt, auf unterschiedlichste Weisen formatiert oder aufgrund schlechter Informationsarchitektur nur mit größerem Aufwand zu erreichen. Grundsätzlich kann die Fülle an Ergebnissen überfordern und nicht immer sind alle relevanten Inhalte über gängige Suchmaschinen auffindbar. Das bedeutet dann oft die mühsame Suche nach den relevanten Informationen in nicht immer nutzerfreundlich gestalteten Datenbanken. Bei so gut wie allen Recherchearbeiten besteht die Aufgabe heutzutage darin, die sprichwörtliche „Nadel im Heuhaufen“ zu finden. In vielen Fällen wünscht man sich einen gewissen Grad an Automatisierung. Daher soll im Rahmen eines Forschungsprojektes an der Hochschule München ein Tool entworfen werden, das Informationen und Inhalte von Webseiten automatisiert gewinnt. 
		\\
		Programmiertechnisch bedient man sich hierfür meistens vorhandener Schnittstellen, die die jeweiligen Quellen zur Verfügung stellen. Neben anwendungsspezifischen APIs sind dies zum Beispiel RSS- oder AtomFeeds, die in eigene Datenbanken geladen und von dort aus weiterverarbeitet werden können. \cite{bib-atomfeed} Die Informationen liegen in diesen Fällen also bereits in einem strukturierten Format vor und enthalten wenig bis gar keinen überflüssigen Inhalt.
		\\
		Hingegen wenn keine der genannten Schnittstellen angeboten werden, bleibt die ständige Beobachtung dieser Inhalte schwierig. Sofern es sich bei den Quellen um einfach strukturierte HTML-Seiten handelt, kann man sich mit verschiedenen Tools weiterhelfen. Gerade bei öffentlichen Datenbanken, deren Inhalte über Webformulare abgefragt werden („Deep Web“), ist dies jedoch meistens nicht möglich. \cite{bib-deepweb} Hier können Webscraper weiterhelfen, die das automatisierte „Auslesen“ von bestimmten strukturierten	oder semi-strukturierten Inhalten aus öffentlich zugänglichen Webseiten ermöglichen. 
		\\
		\newline
		Diese Studienarbeit beleuchtet am Beispiel von TripAdvisor wie ein solcher Webscraper implementiert werden kann. Des Weiteren umfasst diese Studienarbeit das automatisierte Abspeichern der gewonnenen Daten in einer strukturierten Datenbank.
		
		%Gewonnene Daten können so Aufschluss über Verhaltensweisen, Interessen und auch mögliche Kaufanregungen geben.
	
	


	\chapter{Grundlagen}\label{grlagen}
		
		\section{Was ist WebCrawling?}
		
			\subsection{Definition, Erläuterung etc.}
		
			\subsection{Wie funktioniert es? (Allgemeine Beschreibung)}
		
		\section{Tripadvisor}
		
			\subsection{Was ist das für eine Webseite? Kurze Beschreibung}
			
		\section{Was sind strukturierte und unstrukturierte Daten?}
	
	
	

	\chapter{Anforderungen}	
	
		\section{Tabelle: Daten TripAdvisor (Welche Daten holen wir uns? Hotel und Restaurants?) details}
		
		\section{Nicht funktionale Anforderungen: Benutzerfreundlichkeit etc.}
			
			bliblablablablubulskjfvba
			öajsvaölfkvpaijsdv
			\\
			öjydfpvoijaf
			v\\jdofhvöankfv
			aposdfjhvpuaohöjvhb
			
			
			
			
	\chapter{Implementierung}
	
		\section{Konfiguration}
		
			\subsection{Welche Version von Python}
			
			\subsection{Welche Pakete wurden importiert und warum}
			
		\section{Warum BeautifulSoup}
		
		\section{WebCrawler (Wie findet man die Daten auf der Webseite und wie holen wir sie uns.)}
		
			\subsection{RestaurantDaten}
			
			\subsection{Bewertungsdaten}
			
		\section{Datenhaltung}
		%Einfach suche der Daten
			Eine Anforderung an das System besteht auch darin die gewonnenen Daten in einer Datenbank ablegen zu können. Zunächst wurden vom Product Owner die nötigen Ansprüche an die Datenbank bereitgestellt, nach der sich dann aus zwei Datenbanktypen eine vorteilhaftere herauskristallisieren konnte. 
			\\
			Da die gewonnen Rohdaten sich noch meist in einem encodierten Zustand befinden, müssen diese zunächst einen Bereinigungsprozess durchlaufen bevor sie leserlich in der Datenbank abgelegt werden können. Nicht nur das einfach Lesen der Daten sondern auch die Suche nach bestimmten Rechercheergebnissen soll möglichst intuitiv für den Nutzer ablaufen. Die Datenbankstruktur wurde daher Zusammen mit dem Auftraggeber abgestimmt und daraufhin die Architektur implementiert.
		
			\subsection{Definition der Datenbankanforderungen}
			
				Die vom Abnehmer vordefinierten Anforderungen an die Datenbank sind wie folgt:
				
				%Stichpunkte DB-Anforderungen															
				\begin{itemize}
					\item Integritätssicherung\\
					\begin{small}
						Daten werden auf Korrektheit (bereits während der Eingabe) überprüft und Fehlmanipulationen  verhindert 
					\end{small}
					
					\item Redundanzarmut\\
					\begin{small}
						es gibt keine ungeordnete Mehrfachspeicherung von Datenwerten
					\end{small}
								
					\item Datenunabhängigkeit\\
					\begin{small}
						die Datenbank kann ohne Server verwaltet und weiterentwickelt werden
					\end{small} 
					
					\item zentrale Kontrolle\\
					\begin{small}
						ein Administrator ist in der Lage, das gesamte System von einem Rechner aus zu verwalten
					\end{small}
				\end{itemize}
				
				
				\subsubsection{Vergleich: SQLLite und TinyDB}
				
					Im Folgenden werden zwei NoSQOL Datenbanktypen miteinander verglichen, die dem Entwicklerteam als am sinnvollsten erschienen.
					\\
					
					\underline{\textbf{TinyDB:}}
					\newline TinyDB ist eine leichtgewichtige dokumentenorientierte Datenbank, die für eine einfache Bedienung optimiert ist. Dabei ist sie in reinem Python geschrieben und hat keine externen Abhängigkeiten. Das Ziel sind kleine Apps, die von einer SQL-DB oder einem externen Datenbankserver entkoppelt funktionieren.
					\\
					
					\underline{\textbf{SQLite:}}
					\newline SQLite ist eine gemeinfreie Programmbibliothek, die ein relationales Datenbanksystem enthält. Sie unterstützt einen Großteil der im SQL-92-Standard festgelegten SQL-Sprachbefehle. Die SQLite-Bibliothek lässt sich direkt in entsprechende Anwendungen integrieren, sodass keine weitere Server-Software benötigt wird. Dies ist der entscheidende Unterschied zu anderen Datenbanksystemen. Durch das Einbinden der Bibliothek wird die Anwendung um Datenbankfunktionen erweitert, ohne auf externe Softwarepakete angewiesen zu sein.
					\\
					\newline Der wesentliche Vorteil von TinyDB liegt darin, dass diese Datenbank bereits in Python geschrieben ist und sich somit ideal in unseren Source-Code integrieren lässt. Es entstehen dabei keine Inkonsistenzen oder fehlerhafte Bibliotheksaufrufe. Ein weiteres Plus ist die Möglichkeit das Dateiformat JSON als Datenbank zu verwenden. Diese kann im Vergleich zur SQLite Datei in sehr vielen Anwendungen eingebunden und benutzt werden. Zudem ist man zu der Entscheidung gekommen, dass das Programmieren in SQL-Sprache in einem Python-Code zu Verwirrungen und unnötigen Skill-Anforderungen beiträgt, was bei SQLite der Fall ist.
					\\
					Für das Projekt wurde sich einstimmig für die Verwendung von TinyDB als Datenbanksystem entschieden.


			\subsection{Daten bereinigen}
				
				Meist sind die Daten auf den Webseiten bereits kodiert im UTF-8 Muster hinterlegt. UTF-8 (Abk. für 8-Bit UCS Transformation Format, wobei UCS wiederum Universal Character Set abkürzt) ist die am weitesten verbreitete Kodierung für Unicode-Zeichen. Es ist in den ersten 128 Zeichen (Indizes 0–127) deckungsgleich mit ASCII und eignet sich mit in der Regel nur einem Byte Speicherbedarf für Zeichen vieler westlicher Sprachen besonders für die Kodierung englischsprachiger Texte. \cite{bib-utf8} Die bei TripAdvisor angewendete Kodierung ist in dem Fall auch UTF-8 und muss neu decodiert werden, um die deutschen Umlaute korrekt anzeigen zu können.
				
				Nachdem die Rohdaten in Variablen abgelegt wurden, können diese in einer neuen Methode für den Reinigungsprozess weiterverwendet werden.
			
			\subsection{Datenbankstruktur}
				
				Wie viele Tabellen, IDs, Verknüpfungen, Primärschlüssel wurden eingesetzt
			
				\subsubsection{Tabellen mit Spalten}
				
				\subsubsection{Zeichnung Zusammenhang?}
				
		\section{Benutzeroberfläche}
		
			\subsection{Wie die Umsetzung etc.}
			
			\subsection{Lösung mit URL}
	
	


	\chapter{Wurden Anforderungen erfüllt?}		
	
		\section{Ergebnisse}
		
	


	\chapter{Fazit und Ausblick}
	
		\section{Unsere Erfahrungen/Probleme (evtl. API)}

		\section{Wie sind wir damit klargekommen}


 
		
		Programmiersprache: Python
		hihii
				
			\begin{lstlisting} [caption={Registerprogrammierung der Expander}\label{code-i2c-1}, captionpos=b, style=Arduino]
			# -*- coding: utf-8 -*-
			import requests
			import os
			import unicodedata
			import validators
			import threading
			from tkinter import *
			from tkinter import messagebox
			from bs4 import BeautifulSoup
			from setuptools.package_index import HREF
			from tinydb import TinyDB,Query,where
			
			
			class Web_scraping:
			'''
			Created on 17.04.2018
			
			@author: anjawolf
			
			get_single_data: Data will be scraped from the Tripadvisors Website and stored in variables.
			'''
			def get_single_data(self,url):
			source_code = requests.get(url)
			plain_text = source_code.text
			soup = BeautifulSoup(plain_text, "html.parser")
			
			for self.name in soup.find('h1', {'class': 'heading_title'}):
			print("Name:" + self.name.string)
			
			for self.rating_amount in soup.find('span', {'property': 'count'}):
			print("Anzahl Bewertungen:" + self.rating_amount.string)
			
			for self.popularity in soup.find_all('span', {'class': 'header_popularity popIndexValidation'}):
			print("Popularitaet:" + self.popularity.text)
			
			for self.price_level in soup.find('span', {'class': 'header_tags rating_and_popularity'}):
			print("Preis Level:" + self.price_level.string)
			
			for self.cuisine in soup.find_all('span', {'class': 'header_links rating_and_popularity'}):
			print('Kueche:' + self.cuisine.text)
			
			for self.contact_details in soup.find('div', {'class': 'blRow'}):
			print('Kontaktdaten:' + self.contact_details.text)
			
			for self.address in soup.find('span', {'class': 'street-address'}):
			print("Strasse:" + self.address.string)
			
			for self.locality in soup.find('span', {'class': 'locality'}):
			print("PLZ:" + self.locality.string)
			
			for self.phonenumber in soup.find_all('div', {'class': 'blEntry phone'}):
			print("Telefonnummer:" + self.phonenumber.text)
			
						     
			\end{lstlisting}
	
	
	
	
%						    	Hinzufügen relevanter Verzeichnisse
%   									Abbildungsverzeichnis
	\clearpage
	\addcontentsline{toc}{chapter}{Abbildungsverzeichnis}
	\listoffigures
	
	
	
%   									Programmcodeverzeichnis	
	\clearpage
	%Erneute Bennenung für das Verzeichnis 
	\addcontentsline{toc}{chapter}{Programmcodeverzeichnis}
	\renewcommand{\lstlistingname}{Programmcodeverzeichnis}
	\lstlistoflistings
	\renewcommand{\lstlistlistingname}{\lstlistingname}
	
	
	
%   									Abkürzungsverzeichnis	
	\chapter*{Abkürzungsverzeichnis}
		\vspace{1.0cm}
		\begin{acronym}[SEPSEP]
			\acro{oem}[OEM]{Original Equipment Manufacturer}
			\acro{bob}[BOB]{Breakout-Box}
			\acro{can}[CAN]{Controller Area Network}
			\acro{lin}[LIN]{Local Interconnect Network}
			\acro{obd}[OBD]{On-Board-Diagnose}	
			\acro{sda}[SDA]{Serial Data Line}
			\acro{scl}[SCL]{Serial Clock Line}
			\acro{gui}[GUI]{Graphical User Interface}
			\acro{pdf}[PDF]{Portable Document Format}
			%\acro{SD}[SD]{Secure Digital Memory Card}
			%\acro{pc}[PC]{Personal Computer}
			%\acro{USB}[USB]{Universal Serial Bus}
		\end{acronym}
		\addcontentsline{toc}{chapter}{Abkürzungsverzeichnis}
	
	
	
%   									Literaturverzeichnis	
	\clearpage
	\addcontentsline{toc}{chapter}{Literaturverzeichnis}
	\begin{thebibliography}{9} 
		\vspace{1.0cm}
		%Internetrecherche 
		
		\bibitem{bib-atomfeed} Atom Feed\\ Ryte Wiki: Atom Feed, \newline \url{https://de.ryte.com/wiki/Atom_Feed}, aufgerufen am \today
					
		\bibitem{bib-deepweb} Deep Web\\ Advidera: Deep Web, \newline \url{https://www.advidera.com/glossar/deep-web}, aufgerufen am \today	
		
		\bibitem{bib-utf8} Kodierung in UTF-8\\ UTF-8, \newline \url{https://de.wikipedia.org/wiki/UTF-8}, aufgerufen am \today
		
		%Buchrecherche
		%\bibitem{bib-handb} Kraftfahrzeugtechnik \\ Hans-Hermann Braess, Ulrich Seiffert Hrsg: \glqq Vieweg Handbuch Kraftfahrzeugtechnik\grqq, Springer-Vieweg-Verlag, S. 5, 7. Auflage 2013
	\end{thebibliography}
	
	
	
%   									Selbstständigkeitserklärung
	\newpage
	\setstretch{1.5}
	\chapter*{Eidesstattliche Erklärung}
		%\vspace{1.5\textheight}
		Hiermit erklären wir, Johannes Knippel, Anja Wolf, Johanna Sickendiek und Skanny, dass wir die vorliegende Arbeit mit dem Titel \textit{Auslesen und Abspeichern von unstrukturierten Daten am Beispiel von Tripadvisor} selbstständig verfasst, noch nicht anderweitig für Prüfungszwecke vorgelegt, keine anderen als die angegebenen Quellen oder Hilfsmittel benützt sowie wörtliche und sinngemäße Zitate als solche gekennzeichnet haben.
		\vspace{15mm}
		\\
		\\
		\begin{flushleft}
			\begin{tabular}[H]{ll}
				
				München, den \parbox{5,5cm}{\today}  	& 	\parbox{6cm}{\hrule\medskip \textit{Unterschrift}}\\[2cm]
													  	& 	\parbox{6cm}{\hrule\medskip \textit{Unterschrift}}\\[2cm]
													  	& 	\parbox{6cm}{\hrule\medskip \textit{Unterschrift}}\\[2cm]
													  	& 	\parbox{6cm}{\hrule\medskip \textit{Unterschrift}}\\[2cm]
			\end{tabular}
		\end{flushleft}
		\addcontentsline{toc}{chapter}{Eidesstattliche Erklärung}


\end{document}
%The End
%Johannes Knippel